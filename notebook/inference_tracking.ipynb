{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA2C17Rwi34Q",
        "outputId": "eab66f72-e8bc-4e0b-e680-6d35c11bdbe5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ===== PH·∫¶N 1.1: C√ÄI ƒê·∫∂T TH∆Ø VI·ªÜN =====\n",
        "!pip install ultralytics -q\n",
        "!pip install deep-sort-realtime -q\n",
        "!pip install transformers -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9GRseaujCSD",
        "outputId": "e0f9a8a5-83b6-44af-8471-b055b13a24c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===== PH·∫¶N 1.2: T·∫¢I WEIGHTS =====\n",
        "import os\n",
        "\n",
        "WEIGHTS_DIR = 'weights'\n",
        "WEIGHTS_FILE = os.path.join(WEIGHTS_DIR, 'best.pt')  \n",
        "FILE_ID = '1TgD8SUa-hvtHVH-ligQJqrxhzYwzr-Lh'     \n",
        "\n",
        "#\n",
        "os.makedirs(WEIGHTS_DIR, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(WEIGHTS_FILE):\n",
        "    print(f\"üîÑƒêang t·∫£i weights t·ª´ Google Drive v√†o {WEIGHTS_FILE}...\")\n",
        "    try:\n",
        "        import gdown\n",
        "    except ImportError:\n",
        "        !pip install gdown -q\n",
        "        import gdown\n",
        "    \n",
        "    # T·∫£i file\n",
        "    url = f'https://drive.google.com/uc?id={FILE_ID}'\n",
        "    gdown.download(url, WEIGHTS_FILE, quiet=False)\n",
        "    print(\"\\n T·∫£i xong weights!\")\n",
        "else:\n",
        "    print(\" ƒê√£ t√¨m th·∫•y file weights. S·∫µn s√†ng ch·∫°y!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0BOzwq7jPsS",
        "outputId": "e7b1c04f-17df-4142-af09-77d963a44c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
            "üöÄ Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# ===== PH·∫¶N 2: IMPORT V√Ä SETUP =====\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "import gc\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\" Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ultralytics.nn.modules as nn_mod\n",
        "import ultralytics.nn.tasks as y_tasks\n",
        "\n",
        "\n",
        "class CoordAtt(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        mip = max(8, channels // reduction)\n",
        "\n",
        "        self.pool_h = nn.AdaptiveAvgPool2d((None, 1))\n",
        "        self.pool_w = nn.AdaptiveAvgPool2d((1, None))\n",
        "\n",
        "        self.conv1 = nn.Conv2d(channels, mip, kernel_size=1, stride=1, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(mip)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "        self.conv_h = nn.Conv2d(mip, channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv_w = nn.Conv2d(mip, channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        n, c, h, w = x.size()\n",
        "\n",
        "        x_h = self.pool_h(x)\n",
        "        x_w = self.pool_w(x).permute(0, 1, 3, 2)\n",
        "\n",
        "        y = torch.cat([x_h, x_w], dim=2)\n",
        "        y = self.act(self.bn1(self.conv1(y)))\n",
        "\n",
        "        x_h, x_w = torch.split(y, [h, w], dim=2)\n",
        "        x_w = x_w.permute(0, 1, 3, 2)\n",
        "\n",
        "        a_h = torch.sigmoid(self.conv_h(x_h))\n",
        "        a_w = torch.sigmoid(self.conv_w(x_w))\n",
        "\n",
        "        return identity * a_h * a_w\n",
        "\n",
        "\n",
        "nn_mod.CoordAtt = CoordAtt\n",
        "y_tasks.CoordAtt = CoordAtt\n",
        "globals()['CoordAtt'] = CoordAtt\n",
        "\n",
        "print(\" Custom CoordAtt module registered successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6y-8_a0fnLUb"
      },
      "outputs": [],
      "source": [
        "# ===== PH·∫¶N 3: LOAD MODELS =====\n",
        "print(\" Loading YOLO model...\")\n",
        "try:\n",
        "    yolo = YOLO(\"/content/drive/MyDrive/cpv/39_epoch_best8-7.pt\")  # ƒê∆∞·ªùng d·∫´n model c·ªßa b·∫°n trong Drive\n",
        "    print(\" Custom YOLO model loaded\")\n",
        "except:\n",
        "    yolo = YOLO(\"yolov8n.pt\")\n",
        "    print(\" Using default YOLOv8n\")\n",
        "\n",
        "print(\" Loading CLIP model...\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model.eval()\n",
        "print(\" CLIP model loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O82WMwkTtU3G",
        "outputId": "8e78bca1-5ef5-43a0-9b57-0858f67e57fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Text query: a car light orange and yellow\n"
          ]
        }
      ],
      "source": [
        "# ===== PH·∫¶N 4: ENCODE TEXT & IMAGE REFERENCE =====\n",
        "text_query = \"a car light orange and yellow\"\n",
        "print(f\" Text query: {text_query}\")\n",
        "\n",
        "text_inputs = clip_processor(text=[text_query], return_tensors=\"pt\", padding=True).to(device)\n",
        "with torch.no_grad():\n",
        "    text_emb = clip_model.get_text_features(**text_inputs)\n",
        "    text_emb = text_emb / text_emb.norm(p=2, dim=1, keepdim=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1LG5CE4sKQu",
        "outputId": "3b650ee6-3cf9-45ab-9376-44a108ed5fee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Reference embeddings ready\n"
          ]
        }
      ],
      "source": [
        "# Encode ·∫£nh tham chi·∫øu\n",
        "reference_image_path = \"/content/drive/MyDrive/cpv/x_force.jpg\"  # ƒê∆∞·ªùng d·∫´n ·∫£nh xe trong Drive\n",
        "ref_image = Image.open(reference_image_path).convert(\"RGB\").resize((224, 224))\n",
        "ref_inputs = clip_processor(images=[ref_image], return_tensors=\"pt\", padding=True).to(device)\n",
        "with torch.no_grad():\n",
        "    ref_emb = clip_model.get_image_features(**ref_inputs)\n",
        "    ref_emb = ref_emb / ref_emb.norm(p=2, dim=1, keepdim=True)\n",
        "print(\" Reference embeddings ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzdGO5OKOhoZ",
        "outputId": "9c0e8a9e-699f-4b41-f7d3-79a916c745a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ DeepSort tracker initialized\n",
            "üìπ Video: 576x1024 @ 29fps (1870 frames)\n",
            "üìè Resize to: 640x1137\n",
            "\n",
            "üé¨ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω video...\n",
            "============================================================\n",
            "üéØ Frame 50: Found target vehicle - Track ID 2 (Similarity: 0.341)\n",
            "‚è≥ Progress: 2.7% | Targets found: 1\n",
            "üéØ Frame 75: Found target vehicle - Track ID 1 (Similarity: 0.330)\n",
            "üéØ Frame 90: Found target vehicle - Track ID 20 (Similarity: 0.342)\n",
            "‚è≥ Progress: 5.3% | Targets found: 3\n",
            "‚è≥ Progress: 8.0% | Targets found: 3\n",
            "‚è≥ Progress: 10.7% | Targets found: 3\n",
            "üéØ Frame 205: Found target vehicle - Track ID 29 (Similarity: 0.338)\n",
            "üéØ Frame 250: Found target vehicle - Track ID 26 (Similarity: 0.334)\n",
            "‚è≥ Progress: 13.4% | Targets found: 5\n",
            "‚è≥ Progress: 16.0% | Targets found: 5\n",
            "‚è≥ Progress: 18.7% | Targets found: 5\n",
            "üéØ Frame 380: Found target vehicle - Track ID 40 (Similarity: 0.335)\n",
            "‚è≥ Progress: 21.4% | Targets found: 6\n",
            "üéØ Frame 420: Found target vehicle - Track ID 70 (Similarity: 0.349)\n",
            "üéØ Frame 450: Found target vehicle - Track ID 63 (Similarity: 0.337)\n",
            "‚è≥ Progress: 24.1% | Targets found: 8\n",
            "üéØ Frame 470: Found target vehicle - Track ID 57 (Similarity: 0.335)\n",
            "üéØ Frame 485: Found target vehicle - Track ID 67 (Similarity: 0.332)\n",
            "üéØ Frame 490: Found target vehicle - Track ID 66 (Similarity: 0.335)\n",
            "üéØ Frame 495: Found target vehicle - Track ID 71 (Similarity: 0.335)\n",
            "‚è≥ Progress: 26.7% | Targets found: 12\n",
            "‚è≥ Progress: 29.4% | Targets found: 12\n",
            "‚è≥ Progress: 32.1% | Targets found: 12\n",
            "üéØ Frame 625: Found target vehicle - Track ID 113 (Similarity: 0.336)\n",
            "‚è≥ Progress: 34.8% | Targets found: 13\n",
            "‚è≥ Progress: 37.4% | Targets found: 13\n",
            "‚è≥ Progress: 40.1% | Targets found: 13\n",
            "‚è≥ Progress: 42.8% | Targets found: 13\n",
            "‚è≥ Progress: 45.5% | Targets found: 13\n",
            "‚è≥ Progress: 48.1% | Targets found: 13\n",
            "üéØ Frame 945: Found target vehicle - Track ID 167 (Similarity: 0.332)\n",
            "‚è≥ Progress: 50.8% | Targets found: 14\n",
            "üéØ Frame 970: Found target vehicle - Track ID 175 (Similarity: 0.346)\n",
            "üéØ Frame 970: Found target vehicle - Track ID 179 (Similarity: 0.350)\n",
            "üéØ Frame 995: Found target vehicle - Track ID 169 (Similarity: 0.332)\n",
            "‚è≥ Progress: 53.5% | Targets found: 17\n",
            "üéØ Frame 1005: Found target vehicle - Track ID 186 (Similarity: 0.333)\n",
            "üéØ Frame 1035: Found target vehicle - Track ID 170 (Similarity: 0.332)\n",
            "‚è≥ Progress: 56.1% | Targets found: 19\n",
            "üéØ Frame 1070: Found target vehicle - Track ID 192 (Similarity: 0.331)\n",
            "üéØ Frame 1070: Found target vehicle - Track ID 195 (Similarity: 0.332)\n",
            "‚è≥ Progress: 58.8% | Targets found: 21\n",
            "‚è≥ Progress: 61.5% | Targets found: 21\n",
            "‚è≥ Progress: 64.2% | Targets found: 21\n",
            "‚è≥ Progress: 66.8% | Targets found: 21\n",
            "‚è≥ Progress: 69.5% | Targets found: 21\n",
            "‚è≥ Progress: 72.2% | Targets found: 21\n",
            "‚è≥ Progress: 74.9% | Targets found: 21\n",
            "‚è≥ Progress: 77.5% | Targets found: 21\n",
            "üéØ Frame 1460: Found target vehicle - Track ID 251 (Similarity: 0.341)\n",
            "‚è≥ Progress: 80.2% | Targets found: 22\n",
            "üéØ Frame 1520: Found target vehicle - Track ID 235 (Similarity: 0.331)\n",
            "‚è≥ Progress: 82.9% | Targets found: 23\n",
            "‚è≥ Progress: 85.6% | Targets found: 23\n",
            "üéØ Frame 1645: Found target vehicle - Track ID 311 (Similarity: 0.336)\n",
            "‚è≥ Progress: 88.2% | Targets found: 24\n",
            "‚è≥ Progress: 90.9% | Targets found: 24\n",
            "‚è≥ Progress: 93.6% | Targets found: 24\n",
            "‚è≥ Progress: 96.3% | Targets found: 24\n",
            "‚è≥ Progress: 98.9% | Targets found: 24\n",
            "\n",
            "============================================================\n",
            "‚úÖ HO√ÄN TH√ÄNH!\n",
            "üìÅ Video output: /content/drive/MyDrive/cpv/output_tracking_v2.mp4\n",
            "üéØ T·ªïng s·ªë xe c·∫ßn t√¨m ph√°t hi·ªán: 24\n",
            "üìã Track IDs: ['1', '113', '167', '169', '170', '175', '179', '186', '192', '195', '2', '20', '235', '251', '26', '29', '311', '40', '57', '63', '66', '67', '70', '71']\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ===== PH·∫¶N 5: KH·ªûI T·∫†O DEEPSORT TRACKER =====\n",
        "tracker = DeepSort(\n",
        "    max_age=30,           # S·ªë frame t·ªëi ƒëa gi·ªØ track khi m·∫•t\n",
        "    n_init=3,             # S·ªë frame kh·ªüi t·∫°o track\n",
        "    nms_max_overlap=0.7,  # NMS threshold\n",
        "    max_cosine_distance=0.3,\n",
        "    nn_budget=100\n",
        ")\n",
        "print(\" DeepSort tracker initialized\")\n",
        "\n",
        "# ===== PH·∫¶N 6: C·∫§U H√åNH VIDEO =====\n",
        "video_path = \"./inputs/vd_1.mp4\"  # Video input trong Drive\n",
        "output_path = \"./outputs/output_tracking.mp4\"  # Video output\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Resize ƒë·ªÉ gi·∫£m t·∫£i GPU/RAM\n",
        "RESIZE_WIDTH = 640\n",
        "RESIZE_HEIGHT = int(height * (RESIZE_WIDTH / width))\n",
        "\n",
        "out = cv2.VideoWriter(\n",
        "    output_path,\n",
        "    cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "    fps,\n",
        "    (RESIZE_WIDTH, RESIZE_HEIGHT)\n",
        ")\n",
        "\n",
        "print(f\" Video: {width}x{height} @ {fps}fps ({total_frames} frames)\")\n",
        "print(f\" Resize to: {RESIZE_WIDTH}x{RESIZE_HEIGHT}\")\n",
        "\n",
        "# ===== PH·∫¶N 7: C·∫§U H√åNH MATCHING =====\n",
        "THRESHOLD_TEXT = 0.22      # Ng∆∞·ª°ng text similarity\n",
        "THRESHOLD_IMAGE = 0.44     # Ng∆∞·ª°ng image similarity\n",
        "MATCH_INTERVAL = 5        # S·ªë frame check matching m·ªôt l·∫ßn\n",
        "MIN_TRACK_FRAMES = 2       # S·ªë frame t·ªëi thi·ªÉu tr∆∞·ªõc khi match\n",
        "\n",
        "# L∆∞u tr·ªØ tracks v√† matching results\n",
        "tracked_vehicles = {}      # {track_id: {'embeddings': [], 'matched': False, 'best_sim': 0}}\n",
        "target_track_ids = set()   # C√°c track_id ƒë√£ match v·ªõi xe c·∫ßn t√¨m\n",
        "\n",
        "# ===== PH·∫¶N 8: H√ÄM EXTRACT CLIP FEATURES =====\n",
        "@torch.no_grad()\n",
        "def extract_clip_features(crops):\n",
        "    \"\"\"Extract CLIP features cho batch crops\"\"\"\n",
        "    if not crops:\n",
        "        return []\n",
        "\n",
        "    images = [Image.fromarray(cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)).resize((224, 224))\n",
        "              for crop in crops]\n",
        "    inputs = clip_processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "    features = clip_model.get_image_features(**inputs)\n",
        "    features = features / features.norm(p=2, dim=1, keepdim=True)\n",
        "    return features\n",
        "\n",
        "# ===== PH·∫¶N 9: H√ÄM MATCHING =====\n",
        "def check_track_matching(track_id, track_data):\n",
        "    \"\"\"Ki·ªÉm tra track c√≥ match v·ªõi xe c·∫ßn t√¨m kh√¥ng\"\"\"\n",
        "    if track_data['matched'] or len(track_data['embeddings']) < MIN_TRACK_FRAMES:\n",
        "        return False\n",
        "\n",
        "    # L·∫•y embedding trung b√¨nh t·ª´ c√°c frame\n",
        "    avg_emb = torch.mean(torch.stack(track_data['embeddings']), dim=0, keepdim=True)\n",
        "\n",
        "    # T√≠nh similarity v·ªõi text v√† reference image\n",
        "    text_sim = (avg_emb @ text_emb.T).item()\n",
        "    ref_sim = (avg_emb @ ref_emb.T).item()\n",
        "\n",
        "    # Update best similarity\n",
        "    combined_sim = (text_sim + ref_sim) / 2\n",
        "    track_data['best_sim'] = max(track_data['best_sim'], combined_sim)\n",
        "\n",
        "    # Check ng∆∞·ª°ng\n",
        "    if text_sim >= THRESHOLD_TEXT and ref_sim >= THRESHOLD_IMAGE:\n",
        "        track_data['matched'] = True\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# ===== PH·∫¶N 10: V√íNG L·∫∂P CH√çNH =====\n",
        "frame_count = 0\n",
        "process_every_n = 1  # X·ª≠ l√Ω m·ªçi frame (c√≥ th·ªÉ tƒÉng l√™n 2-3 ƒë·ªÉ tƒÉng t·ªëc)\n",
        "\n",
        "print(\"\\nüé¨ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω video...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    frame_count += 1\n",
        "\n",
        "    \n",
        "    if frame_count % process_every_n != 0:\n",
        "        continue\n",
        "\n",
        "    \n",
        "    frame = cv2.resize(frame, (RESIZE_WIDTH, RESIZE_HEIGHT))\n",
        "\n",
        "    # ===== B∆Ø·ªöC 1: YOLO DETECTION =====\n",
        "    results = yolo(frame, conf=0.35, verbose=False)[0]\n",
        "\n",
        "    # ===== B∆Ø·ªöC 2: CHU·∫®N B·ªä DETECTIONS CHO DEEPSORT =====\n",
        "    detections = []\n",
        "    crops = []\n",
        "    bboxes_info = []\n",
        "\n",
        "    for box in results.boxes:\n",
        "        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "        conf = box.conf[0].cpu().numpy()\n",
        "        cls = int(box.cls[0].cpu().numpy())\n",
        "\n",
        "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
        "\n",
        "        # Validate bbox\n",
        "        if x1 >= x2 or y1 >= y2 or x1 < 0 or y1 < 0:\n",
        "            continue\n",
        "\n",
        "        # DeepSORT format: ([left, top, width, height], confidence, class)\n",
        "        detections.append(([x1, y1, x2-x1, y2-y1], conf, cls))\n",
        "\n",
        "        # Crop image cho CLIP\n",
        "        crop = frame[y1:y2, x1:x2]\n",
        "        if crop.size > 0:\n",
        "            crops.append(crop)\n",
        "            bboxes_info.append((x1, y1, x2, y2))\n",
        "\n",
        "    # ===== B∆Ø·ªöC 3: UPDATE TRACKER =====\n",
        "    tracks = tracker.update_tracks(detections, frame=frame)\n",
        "\n",
        "    # ===== B∆Ø·ªöC 4: EXTRACT CLIP FEATURES CHO C√ÅC CROPS =====\n",
        "    if crops and frame_count % MATCH_INTERVAL == 0:\n",
        "        clip_features = extract_clip_features(crops)\n",
        "\n",
        "        # Map features to tracks\n",
        "        feature_idx = 0\n",
        "        for track in tracks:\n",
        "            if not track.is_confirmed():\n",
        "                continue\n",
        "\n",
        "            track_id = track.track_id\n",
        "\n",
        "            # Kh·ªüi t·∫°o track data n·∫øu ch∆∞a c√≥\n",
        "            if track_id not in tracked_vehicles:\n",
        "                tracked_vehicles[track_id] = {\n",
        "                    'embeddings': [],\n",
        "                    'matched': False,\n",
        "                    'best_sim': 0.0\n",
        "                }\n",
        "\n",
        "            # Th√™m embedding m·ªõi (gi·ªõi h·∫°n 20 embeddings g·∫ßn nh·∫•t)\n",
        "            if feature_idx < len(clip_features):\n",
        "                tracked_vehicles[track_id]['embeddings'].append(clip_features[feature_idx])\n",
        "                if len(tracked_vehicles[track_id]['embeddings']) > 20:\n",
        "                    tracked_vehicles[track_id]['embeddings'].pop(0)\n",
        "                feature_idx += 1\n",
        "\n",
        "            # Ki·ªÉm tra matching\n",
        "            if check_track_matching(track_id, tracked_vehicles[track_id]):\n",
        "                target_track_ids.add(track_id)\n",
        "                print(f\"üéØ Frame {frame_count}: Found target vehicle - Track ID {track_id} \"\n",
        "                      f\"(Similarity: {tracked_vehicles[track_id]['best_sim']:.3f})\")\n",
        "\n",
        "    # ===== B∆Ø·ªöC 5: V·∫º BOUNDING BOXES =====\n",
        "    for track in tracks:\n",
        "        if not track.is_confirmed():\n",
        "            continue\n",
        "\n",
        "        track_id = track.track_id\n",
        "        ltrb = track.to_ltrb()\n",
        "        x1, y1, x2, y2 = map(int, ltrb)\n",
        "\n",
        "        # Ch·ªçn m√†u v√† style d·ª±a tr√™n matching\n",
        "        if track_id in target_track_ids:\n",
        "            #  XE C·∫¶N T√åM - M√†u ƒë·ªè, bbox d√†y\n",
        "            color = (0, 0, 255)      # ƒê·ªè\n",
        "            thickness = 4\n",
        "            label = f\"TARGET #{track_id}\"\n",
        "            label_bg = (0, 0, 255)\n",
        "            label_text_color = (255, 255, 255)\n",
        "        else:\n",
        "            # Xe th√¥ng th∆∞·ªùng - M√†u xanh l√°\n",
        "            color = (0, 255, 0)\n",
        "            thickness = 2\n",
        "            label = f\"ID {track_id}\"\n",
        "            label_bg = (0, 255, 0)\n",
        "            label_text_color = (0, 0, 0)\n",
        "\n",
        "        # V·∫Ω bbox\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n",
        "\n",
        "        # V·∫Ω label v·ªõi background\n",
        "        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "        cv2.rectangle(frame, (x1, y1 - h - 10), (x1 + w, y1), label_bg, -1)\n",
        "        cv2.putText(frame, label, (x1, y1 - 5),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, label_text_color, 2)\n",
        "\n",
        "    # ===== B∆Ø·ªöC 6: V·∫º INFO PANEL =====\n",
        "    info_text = [\n",
        "        f\"Frame: {frame_count}/{total_frames}\",\n",
        "        f\"Tracked: {len([t for t in tracks if t.is_confirmed()])}\",\n",
        "        f\"Targets: {len(target_track_ids)}\"\n",
        "    ]\n",
        "\n",
        "    y_offset = 30\n",
        "    for text in info_text:\n",
        "        cv2.putText(frame, text, (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "        y_offset += 30\n",
        "\n",
        "    # ===== B∆Ø·ªöC 7: WRITE FRAME =====\n",
        "    out.write(frame)\n",
        "\n",
        "    # Progress update\n",
        "    if frame_count % 50 == 0:\n",
        "        progress = (frame_count / total_frames) * 100\n",
        "        print(f\" Progress: {progress:.1f}% | Targets found: {len(target_track_ids)}\")\n",
        "\n",
        "    # Gi·∫£i ph√≥ng RAM ƒë·ªãnh k·ª≥\n",
        "    if frame_count % 100 == 0:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# ===== PH·∫¶N 11: CLEANUP =====\n",
        "cap.release()\n",
        "out.release()\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\" HO√ÄN TH√ÄNH!\")\n",
        "print(f\" Video output: {output_path}\")\n",
        "print(f\" T·ªïng s·ªë xe c·∫ßn t√¨m ph√°t hi·ªán: {len(target_track_ids)}\")\n",
        "if target_track_ids:\n",
        "    print(f\" Track IDs: {sorted(target_track_ids)}\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
